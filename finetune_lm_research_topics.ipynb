{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/petersun1937/finetune-lm-research_topics/blob/main/finetune_lm_research_topics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install transformers datasets\n",
        "!pip install -q datasets rouge-score bert-score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pntc94h1PxhZ",
        "outputId": "1cc7d861-f5d6-4614-c19d-078f8b2d5dcd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To ignore WANDB\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "y8TSvHLKQ35_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Finetune LM (Distilgpt2 or TinyMistral, uncomment relevant parts if needed)**"
      ],
      "metadata": {
        "id": "gQfDBtzziWne"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c4e7cc8a252d4f33bc8148e84e347ec5",
            "7f1de113e1b34b3a85bc3053ccf711bf",
            "dd356f3fbe1b43ab892bc48906e7a845",
            "d271bc6b43564f70ac3308d2e502be51",
            "63ec51c37cc746b28affee45c3cfb1b1",
            "5241b31836e849ec94bb8192b3f8f2d3",
            "db31c2cd00314e86bdd259687a2f0d35",
            "3d53db2c6d2a493fa0f3098955549856",
            "5f73daa8929348568d1d36978f2e6cd2",
            "bd6e141e80e2425483709824cf7e7dfe",
            "a7551e0644b341ebb11a319f221b7ec9",
            "55850fd25b884699b40270616ce4a69c",
            "82c8bebb2bea49c2b93a6da1395f2932",
            "92c503e7032b43b1b76a65a57a81d919",
            "fbd13877cb5f46d08bb28b0ff2609f0a",
            "9874ee297c914f17b38a528962635ed8",
            "6f61b358d3cf476d8a5f03bc338270a2",
            "6491b61c1c82460eab48b5887cfd9f02",
            "6c8aef0fc2474d4ead94df804c2741d4",
            "1900181bd6de44c59dcb673e7fa4a4f3",
            "3b420207bb8f455fa427233380435484",
            "a588926c97e740c3986f9f48e08171ec",
            "f8f3cc41ecd94f06a91248756b8e71f1",
            "bd8dd2a567a4467bb1881ea954261e31",
            "24d6a67a96d4445ea825ea8a8215c83c",
            "a6c53f4ad6294ead9156ed287fa676eb",
            "df73f63c4afc4bbb80f6e312a09861e7",
            "319966f934344e52b67f283cd082a237",
            "b4e73c6a3a9f4638bab5e0d514185242",
            "de430adb1e464f0b842c2a6e4120ec49",
            "5c78eafa62c84231b8944f4626682737",
            "47c4cbfa9b034d7b952f18809927e7b4",
            "beac442500c348759c7814727e2b5b85",
            "0a2b93d7598846aaa8e1f26065104b6a",
            "60b9b3695afd4981bcc9a8f4b6a2037f",
            "9e343b2cb02b4a43b83bab2d02652707",
            "c7d0ca68a57843348d6ee706c2f5ce82",
            "91b136b6d37048c3a6496ad44fa6939f",
            "490edfd6be364226b885afc587cb51af",
            "84dc1a421a9a48bc877fd7ecee4344f9",
            "c1e906a3f93e4bfb829d1f4726e07e36",
            "d78996404e2f4641865aea45e31bfa0b",
            "ce4592bea0914d45a0940599ca95dbfc",
            "a5bef8ac10224d73a3496728e78cd231",
            "96bb302962524cd0949b531c42fac785",
            "fc24f8c7a99d4d69a9f2450627a441fc",
            "a678d560a77e407a874beb81fde904ce",
            "a82a1185c1464fbe907450fea7bc33e6",
            "b0e57748ebb844df914e9d8d7aa0267b",
            "d5659cc539244102b63c19aa2b2c2816",
            "b299f9e39e114f78a948e11f5e29208b",
            "abd562b6d44b4fab8c605cb62da1d560",
            "c09130f2914143448a36b506dc2b11c6",
            "ceb732efe4184371b155086df9d77566",
            "97023df258264558a33a40f85db18217",
            "0e20ad76fcef472cb137fc0823c022e5",
            "be6e3260f24840e1afb3ec3d94254eb4",
            "9bfe53fefead45b99509f8da74209aab",
            "e21e90b6a5924ef7af08cbce66128e1f",
            "e2294f0312d94f71b7c8f9c998b20304",
            "68d706bf977a47b7960148ecbd4fa414",
            "6ec22c2e21f5469d8eb721d1665f8a05",
            "e68f7c2a74ec4424be959bf8f6191a25",
            "e00e956095e04ca4b06da7df42e13109",
            "b7104e192bc747e09e87930eeb6cdbed",
            "75107c99c04243a39a87e76410eade71",
            "f87cb6c95970411c8e949ea976b88c68",
            "ce2452bfec734a949cabd33ac5a4b342",
            "e1a683789ad14699b9b36e5b641c2a23",
            "1c1d5a25bd9c4d4e93e3765ab81e3084",
            "1d27942b3f1e4ad6b113c1188a17b33a",
            "4d24bae001164baa8a0cff938d389863",
            "e90de341ba654812ab19ed56f2b47934",
            "acb9131a773345689f0aea85c63e1e93",
            "6b311c8cab754128bd2f704bde7c6991",
            "b5e17b3ab28d4b8781f2bcd6a9cd5344",
            "c5dbbbcc6fed42e6a653beea9d414e9f",
            "50511aa3c00c421abf06a58e66479ded",
            "d6ca8bc21013423382f89d3a1922c8ce",
            "d3f2c19bfe0141d7ab29c12f736eb471",
            "f9f21e00035b4994a8116010ab551018",
            "4da08257048c4d73bb0edcd3ad811709",
            "a993c7158b7e48e297015c41c86e2ab4",
            "903c0c4d70214624a136cc727a1f3295",
            "9e2c8708266f489caca9240d39224648",
            "4fbd0b0a4ed048e3b9a174fa4d4deaa5",
            "84fce2317e794acaa47b5ae550be0899",
            "7789f3ace41242e8a0988497cdce40aa",
            "114d6a3bd2cc474db18b99ee4d66ff7f",
            "a956d7bce2ad494d8a854ef5577737dd",
            "ff18f4505047423c85d09aa2af1af4f9",
            "0d28042d58ba469391513ef99d17615b",
            "223b5104778d42fcb473aeee18512df2",
            "584ff433970d4f3787a766f2175baa3c",
            "f830d270b30744bf939291867ae54391",
            "1f41053e35c24b41b09d89dde820018c",
            "7b15e70f86f2463c897543b09d04c738",
            "4d6926c7bc3542198564c3d0b8c97651",
            "685e9b29fa8e4ae5b9cfc49b120e7848",
            "df47c96c2a3440c68bef4bd8fc235bde",
            "f9b0dffd873c4dc283714da452389c51",
            "ff7d04497c314d0d91449369a2ded8f6",
            "4511109c54d24e2d8da3ea37feb4ceaa",
            "42b310f69eb0438fafca9bb47517365d",
            "879d1b785ed04d08a2eedd2b03655fce",
            "88f347c3d6c548b787bc62f799148f86",
            "110b19dd17d047dfa4dc51f560da4496",
            "afe678e2c998452db8e8c1728180e545",
            "05725eedbce84aafa2fc2556fed9acd8",
            "e1a8bc2ce1794734ae17505c4bb055b9"
          ]
        },
        "id": "8xXBdqC1PvAP",
        "outputId": "f5aef5de-706e-4917-ae8b-0e65fed0d783"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4e7cc8a252d4f33bc8148e84e347ec5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55850fd25b884699b40270616ce4a69c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8f3cc41ecd94f06a91248756b8e71f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a2b93d7598846aaa8e1f26065104b6a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96bb302962524cd0949b531c42fac785"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e20ad76fcef472cb137fc0823c022e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f87cb6c95970411c8e949ea976b88c68"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/657 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50511aa3c00c421abf06a58e66479ded"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "114d6a3bd2cc474db18b99ee4d66ff7f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df47c96c2a3440c68bef4bd8fc235bde"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "<ipython-input-4-df89cf5c77d3>:44: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [150/150 00:15, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>5.642000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>4.853700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>4.413900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>4.156000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>3.957900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>3.523800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>3.531000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>3.508900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>3.533200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.369800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>3.249000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>3.186700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>3.169900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>3.127900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>3.038600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fine-tuned-distilgpt2/tokenizer_config.json',\n",
              " 'fine-tuned-distilgpt2/special_tokens_map.json',\n",
              " 'fine-tuned-distilgpt2/vocab.json',\n",
              " 'fine-tuned-distilgpt2/merges.txt',\n",
              " 'fine-tuned-distilgpt2/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling, AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset\n",
        "# Upload or mount this file into Colab first\n",
        "dataset = load_dataset(\"json\", data_files={\"train\": \"research_nlp_chapters_100.jsonl\"}, split=\"train\")\n",
        "\n",
        "# Preprocess into model input format\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"M4-ai/TinyMistral-248M-v3\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Avoids padding error\n",
        "\n",
        "def format_and_tokenize(example):\n",
        "    prompt = f\"### Problem: {example['input']}\\n### Approach:\"\n",
        "    full_text = f\"{prompt} {example['output']}\"\n",
        "    return tokenizer(full_text, truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "tokenized_dataset = dataset.map(format_and_tokenize)\n",
        "\n",
        "# Load and modify config first\n",
        "config = AutoConfig.from_pretrained(\"M4-ai/TinyMistral-248M-v3\")\n",
        "config.attn_pdrop = 0.1\n",
        "config.resid_pdrop = 0.1\n",
        "\n",
        "# Load model with custom config\n",
        "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
        "#model = AutoModelForCausalLM.from_pretrained(\"M4-ai/TinyMistral-248M-v3\", config=config)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Define TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    save_strategy=\"no\",\n",
        "    logging_steps=10,\n",
        "    fp16=torch.cuda.is_available(),  # Use mixed precision on GPU if possible\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Trainer Setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "\n",
        "# Save model\n",
        "#trainer.save_model(\"fine-tuned-tinyMistral\")\n",
        "#tokenizer.save_pretrained(\"fine-tuned-tinyMistral\")\n",
        "trainer.save_model(\"fine-tuned-distilgpt2\")\n",
        "tokenizer.save_pretrained(\"fine-tuned-distilgpt2\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test fine-tuned model**"
      ],
      "metadata": {
        "id": "sZTwrpygvhun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from rouge_score import rouge_scorer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load Model & Tokenizer\n",
        "#model = AutoModelForCausalLM.from_pretrained(\"fine-tuned-tinyMistral\")\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"fine-tuned-tinyMistral\")\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"fine-tuned-distilgpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"fine-tuned-distilgpt2\")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.eval()\n",
        "\n",
        "# Load Evaluation Dataset\n",
        "eval_data = load_dataset(\"json\", data_files=\"test_research_nlp_chapters_10.jsonl\")[\"train\"]\n",
        "\n",
        "# Load SBERT for Cosine Similarity\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Helper Functions\n",
        "def generate_response(prompt, max_len=128):\n",
        "    input_text = f\"### Problem: {prompt}\\n### Approach:\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_len,\n",
        "        do_sample=True,             # Enable sampling instead of greedy decoding\n",
        "        temperature=0.8             # Lower = conservative, Higher = creative\n",
        "    )\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return decoded.split(\"### Approach:\")[-1].strip()\n",
        "\n",
        "#def repetition_ratio(text, n=5):\n",
        "#    lines = [line.strip() for line in text.split(\"\\n\") if line.strip()]\n",
        "#    return len(lines) / len(set(lines)) if lines else 1.0\n",
        "def repetition_score(text):\n",
        "    lines = [line.strip() for line in text.split(\"\\n\") if line.strip()]\n",
        "    return len(lines) / len(set(lines)) if lines else 1.0\n",
        "\n",
        "# Generate Responses\n",
        "generated_responses, rouge_scores, repetition_scores = [], [], []\n",
        "for ex in eval_data:\n",
        "    prompt = ex[\"input\"]\n",
        "    gold = ex[\"output\"]\n",
        "    gen = generate_response(prompt)\n",
        "\n",
        "    generated_responses.append(gen)\n",
        "\n",
        "    # ROUGE-L\n",
        "    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    rouge_l_f1 = rouge.score(gold, gen)['rougeL'].fmeasure\n",
        "    rouge_scores.append(rouge_l_f1)\n",
        "\n",
        "    # Repetition Score\n",
        "    repetition_scores.append(repetition_score(gen))\n",
        "\n",
        "# Cosine Similarity (batch)\n",
        "gen_embeds = sbert_model.encode(generated_responses, convert_to_tensor=True)\n",
        "ref_embeds = sbert_model.encode([ex[\"output\"] for ex in eval_data], convert_to_tensor=True)\n",
        "cosine_scores = util.cos_sim(gen_embeds, ref_embeds).diagonal().cpu().tolist()\n",
        "\n",
        "# Build DataFrame\n",
        "df = pd.DataFrame({\n",
        "    \"Prompt\": [ex[\"input\"] for ex in eval_data],\n",
        "    \"Generated\": generated_responses,\n",
        "    \"Reference\": [ex[\"output\"] for ex in eval_data],\n",
        "    \"ROUGE-L\": np.round(rouge_scores, 3),\n",
        "    \"CosineSim\": np.round(cosine_scores, 3),\n",
        "    \"RepetitionScore\": np.round(repetition_scores, 2)\n",
        "})\n",
        "\n",
        "# Print Summary\n",
        "print(df[[\"Prompt\", \"Generated\", \"Reference\", \"ROUGE-L\", \"CosineSim\", \"RepetitionScore\"]])\n",
        "print(\"\\nAverage ROUGE-L:\", round(df[\"ROUGE-L\"].mean(), 3))\n",
        "print(\"Average Cosine Similarity:\", round(df[\"CosineSim\"].mean(), 3))\n",
        "print(\"Average Repetition Score:\", round(df[\"RepetitionScore\"].mean(), 2))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fx1dl3K2SFDf",
        "outputId": "80df2ed5-e53e-4f52-93dc-54177ef0338e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Prompt  \\\n",
            "0  How can we correct multilingual grammatical er...   \n",
            "1  What are the best strategies for few-shot abst...   \n",
            "2  How to detect and mitigate bias in generated q...   \n",
            "3  What are current solutions for summarizing ext...   \n",
            "4  How do we model user preferences in dialogue g...   \n",
            "5  What are tokenization-free alternatives for la...   \n",
            "6  What techniques support speech-to-text alignme...   \n",
            "7  How can systems handle topic drift in ongoing ...   \n",
            "8  How to improve answer reranking in hybrid sear...   \n",
            "9  What are viable ways to ground symbolic repres...   \n",
            "\n",
            "                                           Generated  \\\n",
            "0  Train a sample vocabulary model with multiling...   \n",
            "1  Leverage structured question-answer pairs to t...   \n",
            "2  Create a task-length filter that assess and mi...   \n",
            "3  Introduce a summarizing model using a summatio...   \n",
            "4  Train dialogue generation with reinforcement l...   \n",
            "5  Use tokenization-aware tokenization using mult...   \n",
            "6  Apply contextual alignment between low-resourc...   \n",
            "7  Introduce a framework for summarizing topic dr...   \n",
            "8  Introduce a hybrid filter function that pairs ...   \n",
            "9  Train on structured representations using toke...   \n",
            "\n",
            "                                           Reference  ROUGE-L  CosineSim  \\\n",
            "0  Use a multilingual encoder-decoder model equip...    0.096      0.496   \n",
            "1  Implement a retrieval-augmented QA system that...    0.085      0.642   \n",
            "2  Use a two-stage system where question generato...    0.078      0.687   \n",
            "3  Leverage hierarchical encoders with recursive ...    0.085      0.473   \n",
            "4  Represent user behavior with structured profil...    0.100      0.577   \n",
            "5  Train byte-level transformers with learned byt...    0.114      0.377   \n",
            "6  Align self-supervised audio encoders with weak...    0.124      0.497   \n",
            "7  Detect semantic divergence using embedding tra...    0.078      0.506   \n",
            "8  Combine BM25 and dense retriever scores throug...    0.138      0.501   \n",
            "9  Design dual encoders to match structured actio...    0.127      0.496   \n",
            "\n",
            "   RepetitionScore  \n",
            "0              1.0  \n",
            "1              1.0  \n",
            "2              1.0  \n",
            "3              1.0  \n",
            "4              1.0  \n",
            "5              1.0  \n",
            "6              1.0  \n",
            "7              1.0  \n",
            "8              1.0  \n",
            "9              1.0  \n",
            "\n",
            "Average ROUGE-L: 0.102\n",
            "Average Cosine Similarity: 0.525\n",
            "Average Repetition Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test Pretrained Model**"
      ],
      "metadata": {
        "id": "vpzssByglnn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from rouge_score import rouge_scorer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load Pretrained Model & Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
        "\n",
        "#model = AutoModelForCausalLM.from_pretrained(\"M4-ai/TinyMistral-248M-v3\")\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"M4-ai/TinyMistral-248M-v3\")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.eval()\n",
        "\n",
        "# Load Evaluation Dataset\n",
        "eval_data = load_dataset(\"json\", data_files=\"test_research_nlp_chapters_10.jsonl\")[\"train\"]\n",
        "\n",
        "# Load SBERT for Cosine Similarity\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Helper Functions\n",
        "def generate_response(prompt, max_len=128):\n",
        "    input_text = f\"### Problem: {prompt}\\n### Approach:\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_len,\n",
        "        do_sample=True,             # Enable sampling instead of greedy decoding\n",
        "        temperature=0.8             # Lower = conservative, Higher = creative\n",
        "    )\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return decoded.split(\"### Approach:\")[-1].strip()\n",
        "\n",
        "#def repetition_ratio(text, n=5):\n",
        "#    lines = [line.strip() for line in text.split(\"\\n\") if line.strip()]\n",
        "#    return len(lines) / len(set(lines)) if lines else 1.0\n",
        "def repetition_score(text):\n",
        "    lines = [line.strip() for line in text.split(\"\\n\") if line.strip()]\n",
        "    return len(lines) / len(set(lines)) if lines else 1.0\n",
        "\n",
        "# Generate Responses\n",
        "generated_responses, rouge_scores, repetition_scores = [], [], []\n",
        "for ex in eval_data:\n",
        "    prompt = ex[\"input\"]\n",
        "    gold = ex[\"output\"]\n",
        "    gen = generate_response(prompt)\n",
        "\n",
        "    generated_responses.append(gen)\n",
        "\n",
        "    # ROUGE-L\n",
        "    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    rouge_l_f1 = rouge.score(gold, gen)['rougeL'].fmeasure\n",
        "    rouge_scores.append(rouge_l_f1)\n",
        "\n",
        "    # Repetition Score\n",
        "    repetition_scores.append(repetition_score(gen))\n",
        "\n",
        "# Cosine Similarity (batch)\n",
        "gen_embeds = sbert_model.encode(generated_responses, convert_to_tensor=True)\n",
        "ref_embeds = sbert_model.encode([ex[\"output\"] for ex in eval_data], convert_to_tensor=True)\n",
        "cosine_scores = util.cos_sim(gen_embeds, ref_embeds).diagonal().cpu().tolist()\n",
        "\n",
        "# Build DataFrame\n",
        "df = pd.DataFrame({\n",
        "    \"Prompt\": [ex[\"input\"] for ex in eval_data],\n",
        "    \"Generated\": generated_responses,\n",
        "    \"Reference\": [ex[\"output\"] for ex in eval_data],\n",
        "    \"ROUGE-L\": np.round(rouge_scores, 3),\n",
        "    \"CosineSim\": np.round(cosine_scores, 3),\n",
        "    \"RepetitionScore\": np.round(repetition_scores, 2)\n",
        "})\n",
        "\n",
        "# Print Summary\n",
        "print(df[[\"Prompt\", \"Generated\", \"Reference\", \"ROUGE-L\", \"CosineSim\", \"RepetitionScore\"]])\n",
        "print(\"\\nAverage ROUGE-L:\", round(df[\"ROUGE-L\"].mean(), 3))\n",
        "print(\"Average Cosine Similarity:\", round(df[\"CosineSim\"].mean(), 3))\n",
        "print(\"Average Repetition Score:\", round(df[\"RepetitionScore\"].mean(), 2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0o_F2xSzhAH",
        "outputId": "281a0481-1a30-4ae4-991a-b06c532c33f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Prompt  \\\n",
            "0  How can we correct multilingual grammatical er...   \n",
            "1  What are the best strategies for few-shot abst...   \n",
            "2  How to detect and mitigate bias in generated q...   \n",
            "3  What are current solutions for summarizing ext...   \n",
            "4  How do we model user preferences in dialogue g...   \n",
            "5  What are tokenization-free alternatives for la...   \n",
            "6  What techniques support speech-to-text alignme...   \n",
            "7  How can systems handle topic drift in ongoing ...   \n",
            "8  How to improve answer reranking in hybrid sear...   \n",
            "9  What are viable ways to ground symbolic repres...   \n",
            "\n",
            "                                           Generated  \\\n",
            "0  To understand grammatical errors, we can look ...   \n",
            "1  The question is: How do you respond to a given...   \n",
            "2  To detect and mitigate bias in generated quest...   \n",
            "3  You must be very careful with your formatting ...   \n",
            "4  The answer to this is simple. To do so we need...   \n",
            "5  The problem is that the tokenization solution ...   \n",
            "6  The use of a standard library to support multi...   \n",
            "7  To find a solution for these issues, you shoul...   \n",
            "8  Reranking is not so easy for most search engin...   \n",
            "9  You can simply place the symbols in the symbol...   \n",
            "\n",
            "                                           Reference  ROUGE-L  CosineSim  \\\n",
            "0  Use a multilingual encoder-decoder model equip...    0.061      0.396   \n",
            "1  Implement a retrieval-augmented QA system that...    0.072      0.449   \n",
            "2  Use a two-stage system where question generato...    0.092      0.630   \n",
            "3  Leverage hierarchical encoders with recursive ...    0.030      0.081   \n",
            "4  Represent user behavior with structured profil...    0.059      0.321   \n",
            "5  Train byte-level transformers with learned byt...    0.015      0.198   \n",
            "6  Align self-supervised audio encoders with weak...    0.016      0.045   \n",
            "7  Detect semantic divergence using embedding tra...    0.050      0.232   \n",
            "8  Combine BM25 and dense retriever scores throug...    0.062      0.480   \n",
            "9  Design dual encoders to match structured actio...    0.017      0.044   \n",
            "\n",
            "   RepetitionScore  \n",
            "0             1.00  \n",
            "1             1.00  \n",
            "2             5.00  \n",
            "3             1.00  \n",
            "4             1.00  \n",
            "5             1.00  \n",
            "6             1.17  \n",
            "7             1.83  \n",
            "8             1.00  \n",
            "9             1.00  \n",
            "\n",
            "Average ROUGE-L: 0.047\n",
            "Average Cosine Similarity: 0.288\n",
            "Average Repetition Score: 1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LoRA (performance not good)**"
      ],
      "metadata": {
        "id": "d3fOsbFuqtMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q peft datasets transformers accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "jL2hUUdfriUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling)\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# Load Dataset\n",
        "dataset = load_dataset(\"json\", data_files={\"train\": \"research_nlp_chapters_100.jsonl\"}, split=\"train\")\n",
        "\n",
        "# Tokenizer & Prompt Formatting\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"M4-ai/TinyMistral-248M-v3\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def format_and_tokenize(example):\n",
        "    prompt = f\"### Problem: {example['input']}\\n### Approach:\"\n",
        "    full_text = f\"{prompt} {example['output']}\"\n",
        "    return tokenizer(full_text, truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "tokenized_dataset = dataset.map(format_and_tokenize, remove_columns=dataset.column_names)\n",
        "\n",
        "# Load Base Model and Apply LoRA\n",
        "config = AutoConfig.from_pretrained(\"M4-ai/TinyMistral-248M-v3\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"M4-ai/TinyMistral-248M-v3\", config=config)\n",
        "base_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Adapt based on model internals\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "model.print_trainable_parameters()  # Optional: confirm LoRA is active\n",
        "\n",
        "# TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_lora\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Trainer Setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "# Train & Save\n",
        "trainer.train()\n",
        "trainer.save_model(\"fine-tuned-lora-TinyMistral-248M-v3\")\n",
        "tokenizer.save_pretrained(\"fine-tuned-lora-TinyMistral-248M-v3\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ucbZ3DJ9qskK",
        "outputId": "ee5211be-888d-4b50-d64f-1f086ea3e8f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 319,488 || all params: 248,343,552 || trainable%: 0.1286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-133f434284b1>:56: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [300/300 00:23, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>5.360600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>5.431600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>5.199700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>5.379100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>5.342200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>5.208000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>5.080400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>4.975500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>4.989200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>4.970900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>5.000200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>4.941100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>5.042200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>4.741500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>4.819200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>4.690100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>4.564100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>4.951300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>4.845300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>4.740300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>4.877000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>4.544800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>4.457800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>4.657300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>4.645800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>4.514000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>4.681900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>4.672600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>4.673900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>4.401700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fine-tuned-lora-TinyMistral-248M-v3/tokenizer_config.json',\n",
              " 'fine-tuned-lora-TinyMistral-248M-v3/special_tokens_map.json',\n",
              " 'fine-tuned-lora-TinyMistral-248M-v3/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score as bertscore\n",
        "\n",
        "# Load Fine-Tuned Model\n",
        "model_path = \"fine-tuned-lora-TinyMistral-248M-v3\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.eval()\n",
        "\n",
        "# Load Test Dataset\n",
        "eval_data = load_dataset(\"json\", data_files=\"test_research_nlp_chapters_10.jsonl\")[\"train\"]\n",
        "\n",
        "def generate_response(prompt, max_len=128):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_len,\n",
        "        do_sample=False,\n",
        "        repetition_penalty=1.2,  # Helps reduce output loops\n",
        "        temperature=0.8,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return decoded.strip()\n",
        "\n",
        "# Initialize Metrics\n",
        "rouge = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
        "bleu_scores, rouge_scores, preds, refs = [], [], [], []\n",
        "\n",
        "# Run Evaluation\n",
        "for ex in eval_data:\n",
        "    gold = ex[\"output\"]\n",
        "    prompt = ex[\"input\"]\n",
        "    gen = generate_response(prompt)\n",
        "\n",
        "    # Print example\n",
        "    print(\"\\n---\")\n",
        "    print(\"Prompt:\", prompt)\n",
        "    print(\"Generated:\", gen)\n",
        "    print(\"Reference:\", gold)\n",
        "\n",
        "    # BLEU\n",
        "    ref_tokens = [gold.split()]\n",
        "    pred_tokens = gen.split()\n",
        "    bleu = sentence_bleu(ref_tokens, pred_tokens)\n",
        "    bleu_scores.append(bleu)\n",
        "\n",
        "    # ROUGE-L\n",
        "    rouge_L = rouge.score(gold, gen)[\"rougeL\"].fmeasure\n",
        "    rouge_scores.append(rouge_L)\n",
        "\n",
        "    preds.append(gen)\n",
        "    refs.append(gold)\n",
        "\n",
        "# BERTScore\n",
        "P, R, F1 = bertscore(preds, refs, lang=\"en\", rescale_with_baseline=True)\n",
        "\n",
        "# Print Metrics\n",
        "print(\"\\n--- Evaluation Summary ---\")\n",
        "print(f\"Average BLEU:      {sum(bleu_scores)/len(bleu_scores):.3f}\")\n",
        "print(f\"Average ROUGE-L:   {sum(rouge_scores)/len(rouge_scores):.3f}\")\n",
        "print(f\"Average BERTScore-F1: {F1.mean().item():.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mh0Ok1ApvWER",
        "outputId": "d7dc792d-0fd9-4314-9e2e-4808792e28a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---\n",
            "Prompt: How can we correct multilingual grammatical errors effectively?\n",
            "Generated: How can we correct multilingual grammatical errors effectively?\n",
            "The aim of this paper is to develop a new approach for translating phonetic expressions into English. The purpose of the study was to investigate whether phonological pronunciation and vocabulary are related in different ways, as well as how these differences affect language acquisition. We found that phonemic speech patterns differed between languages from native speakers (English) and non-native speakers (English). Our results suggest that phonemes may be more sensitive than linguists because they have higher levels of phonemes than other phonemes. This suggests that phonemes might also exhibit lower phoneme\n",
            "Reference: Use a multilingual encoder-decoder model equipped with language-specific adapters, trained on synthetic grammatical errors derived from morphosyntactic templates. Integrate beam rescoring modules using monolingual fluency discriminators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---\n",
            "Prompt: What are the best strategies for few-shot abstractive question answering?\n",
            "Generated: What are the best strategies for few-shot abstractive question answering?\n",
            "The following is a list of questions that can be answered by asking:\n",
            "\n",
            "1. How do you define an effective methodology in which to evaluate and analyze data?\n",
            "2. How does one describe a strategy based on objective criteria, such as accuracy or consistency?\n",
            "3. How should we assess whether a tool has been used successfully in practice?\n",
            "4. How should we measure effectiveness when using tools with high quality results?\n",
            "5. How should our research approach compare to other methods?\n",
            "6. How should we use metrics to improve performance?\n",
            "7. How should we apply statistical analysis techniques to quantify differences between\n",
            "Reference: Implement a retrieval-augmented QA system that uses example-driven prompt generation. Incorporate contrastive representation learning between support examples and candidate answers for better alignment.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---\n",
            "Prompt: How to detect and mitigate bias in generated questions?\n",
            "Generated: How to detect and mitigate bias in generated questions?\n",
            "In this paper, we present a novel method for predicting bias based on the following data: 1) The probability of being falsely accused by an individual is reduced when he or she has been identified as guilty. This means that if one suspects another person’s guilt, then it will be considered false positives (i.e., false negatives). We show here that implicitly discriminating against individuals who are not innocent does not necessarily lead to negative outcomes. In addition, our results suggest that implicit prejudice may increase risky behavioral behaviors among people with low self-esteem. Our findings indicate that unconscious\n",
            "Reference: Use a two-stage system where question generators are followed by bias detectors trained on linguistically annotated cues. Introduce a debiasing layer that rephrases problematic patterns via conditional rewriting.\n",
            "\n",
            "---\n",
            "Prompt: What are current solutions for summarizing extremely long articles?\n",
            "Generated: What are current solutions for summarizing extremely long articles?\n",
            "The short answer is that the longer article, the more likely it will be to provide a comprehensive overview of the literature. The shorter version may not have sufficient information and therefore should not be considered as an exhaustive summary. It can also be used in conjunction with other sources such as research papers or reports from academic journals.\n",
            "\n",
            "In this paper we propose a methodology based on statistical analysis which uses data collected by various scientific publications (such as peer-reviewed studies) to analyze the effects of different types of news stories published during 2015. We then use these methods to examine how they affect readers'\n",
            "Reference: Leverage hierarchical encoders with recursive summarization checkpoints. Train segment summarizers with temporal coherence loss and fuse outputs using learned summary discourse graphs.\n",
            "\n",
            "---\n",
            "Prompt: How do we model user preferences in dialogue generation?\n",
            "Generated: How do we model user preferences in dialogue generation?\n",
            "In this paper, we propose a new approach to the problem of determining whether users are willing to engage with content. We show that by using an algorithmic decision-making process, consumers can identify and respond appropriately to different types of content (e.g., video clips) without having to rely on external sources or methods. This results in better understanding of how people interact with each other through interactions between their audiences and advertisers. Finally, it is possible for individuals to learn more about what they're watching and why they want to watch them. In addition, our research shows that social media platforms have potential applications beyond advertising\n",
            "Reference: Represent user behavior with structured profiles and dynamically embed these during decoding. Fine-tune generators on persona-specific dialogues and optimize for controllability using reinforcement from human feedback.\n",
            "\n",
            "---\n",
            "Prompt: What are tokenization-free alternatives for language modeling?\n",
            "Generated: What are tokenization-free alternatives for language modeling?\n",
            "In this paper, we propose a new approach to the problem of tokenizing languages. We introduce an algorithm that uses tokens and tokens as input sources in order to generate quantitative data on the basis of these tokens. The model is based on the concept of “quantum” cryptography (QKD). This method allows us to analyze the behavior of tokens by analyzing their performance characteristics. By using QKD, users can identify potential problems with their own identity, such as fraudulent transactions or misuse of tokens. In addition, it will be possible to verify whether any transaction has been made without validating its authenticity\n",
            "Reference: Train byte-level transformers with learned byte-pair positional offsets. Replace subword boundaries with entropy-based chunk segmentation and evaluate on multilingual datasets with varied script lengths.\n",
            "\n",
            "---\n",
            "Prompt: What techniques support speech-to-text alignment in low-resource languages?\n",
            "Generated: What techniques support speech-to-text alignment in low-resource languages?\n",
            "In this paper, we propose a method for identifying and analyzing language expressions. We present the proposed approach to address these problems by using an algorithm that uses neural networks as input agents. Our model is based on the concept of semantic recognition: it allows us to identify words with specific contexts (e.g., sentences) without having to explicitly label them. The results show how linguistic processing can be used to improve comprehension skills across different domains. Finally, our system will enable students to learn more about grammar concepts through interactive learning experiences.\n",
            "Reference: Align self-supervised audio encoders with weakly-labeled phoneme spans via CTC alignment heads. Boost alignment accuracy using cross-lingual phonetic knowledge transfer from high-resource anchors.\n",
            "\n",
            "---\n",
            "Prompt: How can systems handle topic drift in ongoing conversations?\n",
            "Generated: How can systems handle topic drift in ongoing conversations?\n",
            "The following is a discussion of the current state of affairs within organizations. The authors have been involved with various topics, including:\n",
            "\n",
            "• What are some common problems that arise when working on an organization's internal communications strategy?\n",
            "• Why do they occur and why should we care about them?\n",
            "• How to manage communication between stakeholders and management teams.\n",
            "• How to communicate effectively with clients who share their concerns and needs.\n",
            "• How to organize meetings among stakeholders and partners.\n",
            "• How to coordinate meetings with stakeholder groups.\n",
            "• How to monitor and respond to external events.\n",
            "• How to maintain\n",
            "Reference: Detect semantic divergence using embedding trajectory tracking across turns. Apply a gating mechanism to re-anchor the conversation state using topic prompts or clarification questions.\n",
            "\n",
            "---\n",
            "Prompt: How to improve answer reranking in hybrid search engines?\n",
            "Generated: How to improve answer reranking in hybrid search engines?\n",
            "The following is a list of questions that can help you find the answers. The questionnaire will be used for each topic, and it should contain information about your specific needs or requirements. It may also include recommendations on how to apply these criteria to your application.\n",
            "In this section we'll discuss some common topics related to driving directions, including:\n",
            "• What are the most important factors when choosing an automobile insurance policy?\n",
            "• Why do drivers choose auto insurance policies based on their preferences?\n",
            "• How does car insurance affect driver satisfaction?\n",
            "• How do vehicle insurance plans compare with other types of insurance coverage?\n",
            "•\n",
            "Reference: Combine BM25 and dense retriever scores through a learned gating function sensitive to query complexity. Apply contextual reranking using BERT-based cross-encoders trained on click-through relevance.\n",
            "\n",
            "---\n",
            "Prompt: What are viable ways to ground symbolic representations in instruction tasks?\n",
            "Generated: What are viable ways to ground symbolic representations in instruction tasks?\n",
            "In this paper, we propose a method of constructing an abstract representation for the expression of symbols and expressions. We present a simple example that illustrates how students can use these concepts to create their own speech recognition systems. The student will be able to understand the conceptual structure of speech recognition system using examples from various sources. Finally, we show how our approach is applied to reinforce language learning by providing practical examples with context-dependent methods. Our proposed model allows us to apply linguistic theory principles to address complex problem solving problems such as grammatical ambiguity, sentence substitution, and semantic reasoning. By combining these approaches,\n",
            "Reference: Design dual encoders to match structured action graphs with natural instruction spans. Supervise training with alignment labels and enforce consistency between predicted steps and goal states.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluation Summary ---\n",
            "Average BLEU:      0.000\n",
            "Average ROUGE-L:   0.060\n",
            "Average BERTScore-F1: -0.045\n"
          ]
        }
      ]
    }
  ]
}