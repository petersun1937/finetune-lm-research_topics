{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/petersun1937/finetune-lm-research_topics/blob/main/finetune_lm_research_topics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pntc94h1PxhZ"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install transformers datasets\n",
        "!pip install -q datasets rouge-score bert-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8TSvHLKQ35_"
      },
      "outputs": [],
      "source": [
        "# To ignore WANDB\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQfDBtzziWne"
      },
      "source": [
        "# **Finetune LM (Distilgpt2 or TinyMistral, uncomment relevant parts if needed)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xXBdqC1PvAP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling, AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset\n",
        "# Upload or mount this file into Colab first\n",
        "dataset = load_dataset(\"json\", data_files={\"train\": \"research_nlp_chapters_100.jsonl\"}, split=\"train\")\n",
        "\n",
        "# Preprocess into model input format\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"M4-ai/TinyMistral-248M-v3\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Avoids padding error\n",
        "\n",
        "def format_and_tokenize(example):\n",
        "    prompt = f\"### Problem: {example['input']}\\n### Approach:\"\n",
        "    full_text = f\"{prompt} {example['output']}\"\n",
        "    return tokenizer(full_text, truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "tokenized_dataset = dataset.map(format_and_tokenize)\n",
        "\n",
        "# Load and modify config first\n",
        "config = AutoConfig.from_pretrained(\"M4-ai/TinyMistral-248M-v3\")\n",
        "config.attn_pdrop = 0.1\n",
        "config.resid_pdrop = 0.1\n",
        "\n",
        "# Load model with custom config\n",
        "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
        "#model = AutoModelForCausalLM.from_pretrained(\"M4-ai/TinyMistral-248M-v3\", config=config)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Define TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    save_strategy=\"no\",\n",
        "    logging_steps=10,\n",
        "    fp16=torch.cuda.is_available(),  # Use mixed precision on GPU if possible\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Trainer Setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "\n",
        "# Save model\n",
        "#trainer.save_model(\"fine-tuned-tinyMistral\")\n",
        "#tokenizer.save_pretrained(\"fine-tuned-tinyMistral\")\n",
        "trainer.save_model(\"fine-tuned-distilgpt2\")\n",
        "tokenizer.save_pretrained(\"fine-tuned-distilgpt2\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZTwrpygvhun"
      },
      "source": [
        "# **Test fine-tuned model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fx1dl3K2SFDf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from rouge_score import rouge_scorer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load Model & Tokenizer\n",
        "#model = AutoModelForCausalLM.from_pretrained(\"fine-tuned-tinyMistral\")\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"fine-tuned-tinyMistral\")\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"fine-tuned-distilgpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"fine-tuned-distilgpt2\")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.eval()\n",
        "\n",
        "# Load Evaluation Dataset\n",
        "eval_data = load_dataset(\"json\", data_files=\"test_research_nlp_chapters_10.jsonl\")[\"train\"]\n",
        "\n",
        "# Load SBERT for Cosine Similarity\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Helper Functions\n",
        "def generate_response(prompt, max_len=128):\n",
        "    input_text = f\"### Problem: {prompt}\\n### Approach:\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_len,\n",
        "        do_sample=True,             # Enable sampling instead of greedy decoding\n",
        "        temperature=0.8             # Lower = conservative, Higher = creative\n",
        "    )\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return decoded.split(\"### Approach:\")[-1].strip()\n",
        "\n",
        "#def repetition_ratio(text, n=5):\n",
        "#    lines = [line.strip() for line in text.split(\"\\n\") if line.strip()]\n",
        "#    return len(lines) / len(set(lines)) if lines else 1.0\n",
        "def repetition_score(text):\n",
        "    lines = [line.strip() for line in text.split(\"\\n\") if line.strip()]\n",
        "    return len(lines) / len(set(lines)) if lines else 1.0\n",
        "\n",
        "# Generate Responses\n",
        "generated_responses, rouge_scores, repetition_scores = [], [], []\n",
        "for ex in eval_data:\n",
        "    prompt = ex[\"input\"]\n",
        "    gold = ex[\"output\"]\n",
        "    gen = generate_response(prompt)\n",
        "\n",
        "    generated_responses.append(gen)\n",
        "\n",
        "    # ROUGE-L\n",
        "    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    rouge_l_f1 = rouge.score(gold, gen)['rougeL'].fmeasure\n",
        "    rouge_scores.append(rouge_l_f1)\n",
        "\n",
        "    # Repetition Score\n",
        "    repetition_scores.append(repetition_score(gen))\n",
        "\n",
        "# Cosine Similarity (batch)\n",
        "gen_embeds = sbert_model.encode(generated_responses, convert_to_tensor=True)\n",
        "ref_embeds = sbert_model.encode([ex[\"output\"] for ex in eval_data], convert_to_tensor=True)\n",
        "cosine_scores = util.cos_sim(gen_embeds, ref_embeds).diagonal().cpu().tolist()\n",
        "\n",
        "# Build DataFrame\n",
        "df = pd.DataFrame({\n",
        "    \"Prompt\": [ex[\"input\"] for ex in eval_data],\n",
        "    \"Generated\": generated_responses,\n",
        "    \"Reference\": [ex[\"output\"] for ex in eval_data],\n",
        "    \"ROUGE-L\": np.round(rouge_scores, 3),\n",
        "    \"CosineSim\": np.round(cosine_scores, 3),\n",
        "    \"RepetitionScore\": np.round(repetition_scores, 2)\n",
        "})\n",
        "\n",
        "# Print Summary\n",
        "print(df[[\"Prompt\", \"Generated\", \"Reference\", \"ROUGE-L\", \"CosineSim\", \"RepetitionScore\"]])\n",
        "print(\"\\nAverage ROUGE-L:\", round(df[\"ROUGE-L\"].mean(), 3))\n",
        "print(\"Average Cosine Similarity:\", round(df[\"CosineSim\"].mean(), 3))\n",
        "print(\"Average Repetition Score:\", round(df[\"RepetitionScore\"].mean(), 2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpzssByglnn9"
      },
      "source": [
        "# **Test Pretrained Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0o_F2xSzhAH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from rouge_score import rouge_scorer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load Pretrained Model & Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
        "\n",
        "#model = AutoModelForCausalLM.from_pretrained(\"M4-ai/TinyMistral-248M-v3\")\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"M4-ai/TinyMistral-248M-v3\")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.eval()\n",
        "\n",
        "# Load Evaluation Dataset\n",
        "eval_data = load_dataset(\"json\", data_files=\"test_research_nlp_chapters_10.jsonl\")[\"train\"]\n",
        "\n",
        "# Load SBERT for Cosine Similarity\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Helper Functions\n",
        "def generate_response(prompt, max_len=128):\n",
        "    input_text = f\"### Problem: {prompt}\\n### Approach:\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_len,\n",
        "        do_sample=True,             # Enable sampling instead of greedy decoding\n",
        "        temperature=0.8             # Lower = conservative, Higher = creative\n",
        "    )\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return decoded.split(\"### Approach:\")[-1].strip()\n",
        "\n",
        "#def repetition_ratio(text, n=5):\n",
        "#    lines = [line.strip() for line in text.split(\"\\n\") if line.strip()]\n",
        "#    return len(lines) / len(set(lines)) if lines else 1.0\n",
        "def repetition_score(text):\n",
        "    lines = [line.strip() for line in text.split(\"\\n\") if line.strip()]\n",
        "    return len(lines) / len(set(lines)) if lines else 1.0\n",
        "\n",
        "# Generate Responses\n",
        "generated_responses, rouge_scores, repetition_scores = [], [], []\n",
        "for ex in eval_data:\n",
        "    prompt = ex[\"input\"]\n",
        "    gold = ex[\"output\"]\n",
        "    gen = generate_response(prompt)\n",
        "\n",
        "    generated_responses.append(gen)\n",
        "\n",
        "    # ROUGE-L\n",
        "    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    rouge_l_f1 = rouge.score(gold, gen)['rougeL'].fmeasure\n",
        "    rouge_scores.append(rouge_l_f1)\n",
        "\n",
        "    # Repetition Score\n",
        "    repetition_scores.append(repetition_score(gen))\n",
        "\n",
        "# Cosine Similarity (batch)\n",
        "gen_embeds = sbert_model.encode(generated_responses, convert_to_tensor=True)\n",
        "ref_embeds = sbert_model.encode([ex[\"output\"] for ex in eval_data], convert_to_tensor=True)\n",
        "cosine_scores = util.cos_sim(gen_embeds, ref_embeds).diagonal().cpu().tolist()\n",
        "\n",
        "# Build DataFrame\n",
        "df = pd.DataFrame({\n",
        "    \"Prompt\": [ex[\"input\"] for ex in eval_data],\n",
        "    \"Generated\": generated_responses,\n",
        "    \"Reference\": [ex[\"output\"] for ex in eval_data],\n",
        "    \"ROUGE-L\": np.round(rouge_scores, 3),\n",
        "    \"CosineSim\": np.round(cosine_scores, 3),\n",
        "    \"RepetitionScore\": np.round(repetition_scores, 2)\n",
        "})\n",
        "\n",
        "# Print Summary\n",
        "print(df[[\"Prompt\", \"Generated\", \"Reference\", \"ROUGE-L\", \"CosineSim\", \"RepetitionScore\"]])\n",
        "print(\"\\nAverage ROUGE-L:\", round(df[\"ROUGE-L\"].mean(), 3))\n",
        "print(\"Average Cosine Similarity:\", round(df[\"CosineSim\"].mean(), 3))\n",
        "print(\"Average Repetition Score:\", round(df[\"RepetitionScore\"].mean(), 2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3fOsbFuqtMT"
      },
      "source": [
        "# **LoRA (performance not good)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jL2hUUdfriUh"
      },
      "outputs": [],
      "source": [
        "!pip install -q peft datasets transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucbZ3DJ9qskK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling)\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# Load Dataset\n",
        "dataset = load_dataset(\"json\", data_files={\"train\": \"research_nlp_chapters_100.jsonl\"}, split=\"train\")\n",
        "\n",
        "# Tokenizer & Prompt Formatting\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"M4-ai/TinyMistral-248M-v3\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def format_and_tokenize(example):\n",
        "    prompt = f\"### Problem: {example['input']}\\n### Approach:\"\n",
        "    full_text = f\"{prompt} {example['output']}\"\n",
        "    return tokenizer(full_text, truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "tokenized_dataset = dataset.map(format_and_tokenize, remove_columns=dataset.column_names)\n",
        "\n",
        "# Load Base Model and Apply LoRA\n",
        "config = AutoConfig.from_pretrained(\"M4-ai/TinyMistral-248M-v3\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"M4-ai/TinyMistral-248M-v3\", config=config)\n",
        "base_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Adapt based on model internals\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "model.print_trainable_parameters()  # Optional: confirm LoRA is active\n",
        "\n",
        "# TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_lora\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Trainer Setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "# Train & Save\n",
        "trainer.train()\n",
        "trainer.save_model(\"fine-tuned-lora-TinyMistral-248M-v3\")\n",
        "tokenizer.save_pretrained(\"fine-tuned-lora-TinyMistral-248M-v3\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mh0Ok1ApvWER"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score as bertscore\n",
        "\n",
        "# Load Fine-Tuned Model\n",
        "model_path = \"fine-tuned-lora-TinyMistral-248M-v3\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.eval()\n",
        "\n",
        "# Load Test Dataset\n",
        "eval_data = load_dataset(\"json\", data_files=\"test_research_nlp_chapters_10.jsonl\")[\"train\"]\n",
        "\n",
        "def generate_response(prompt, max_len=128):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_len,\n",
        "        do_sample=False,\n",
        "        repetition_penalty=1.2,  # Helps reduce output loops\n",
        "        temperature=0.8,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return decoded.strip()\n",
        "\n",
        "# Initialize Metrics\n",
        "rouge = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
        "bleu_scores, rouge_scores, preds, refs = [], [], [], []\n",
        "\n",
        "# Run Evaluation\n",
        "for ex in eval_data:\n",
        "    gold = ex[\"output\"]\n",
        "    prompt = ex[\"input\"]\n",
        "    gen = generate_response(prompt)\n",
        "\n",
        "    # Print example\n",
        "    print(\"\\n---\")\n",
        "    print(\"Prompt:\", prompt)\n",
        "    print(\"Generated:\", gen)\n",
        "    print(\"Reference:\", gold)\n",
        "\n",
        "    # BLEU\n",
        "    ref_tokens = [gold.split()]\n",
        "    pred_tokens = gen.split()\n",
        "    bleu = sentence_bleu(ref_tokens, pred_tokens)\n",
        "    bleu_scores.append(bleu)\n",
        "\n",
        "    # ROUGE-L\n",
        "    rouge_L = rouge.score(gold, gen)[\"rougeL\"].fmeasure\n",
        "    rouge_scores.append(rouge_L)\n",
        "\n",
        "    preds.append(gen)\n",
        "    refs.append(gold)\n",
        "\n",
        "# BERTScore\n",
        "P, R, F1 = bertscore(preds, refs, lang=\"en\", rescale_with_baseline=True)\n",
        "\n",
        "# Print Metrics\n",
        "print(\"\\n--- Evaluation Summary ---\")\n",
        "print(f\"Average BLEU:      {sum(bleu_scores)/len(bleu_scores):.3f}\")\n",
        "print(f\"Average ROUGE-L:   {sum(rouge_scores)/len(rouge_scores):.3f}\")\n",
        "print(f\"Average BERTScore-F1: {F1.mean().item():.3f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}